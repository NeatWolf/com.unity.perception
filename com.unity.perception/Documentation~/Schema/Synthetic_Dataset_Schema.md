# Synthetic Dataset Schema
 
This document describes the schema used to store synthetic datasets generated with [USim](https://unity.com/products/simulation) (Unity Simulations).
This schema allows the raw data generated by USim to be transformed into a structure which can be consumed by machine 
learning models or easily parsed to show statistics within the dataset.
Synthetic datasets are typically composed of sensor captured data and annotations e.g. images and 2d bounding box labels. 
These data come in various forms and might be captured by different sensors and annotation mechanisms that were 
triggered at different frequencies this schema organizes all of the data into a single dataset. 

**todo is it necessary to share our goals of the schema in this?** 
In this document, we will discuss a lightweight dataset schema design to organize synthetic datasets. We designed this 
schema with the following goals in mind:

* Design a dataset schema that can be used to organize captured sensor data and annotations from a simulation. 
This schema allows us to maintain a contract between the dataset producer (e.g. Perception SDK) and consumers 
(e.g. Statistics and ML Modeling...)

* This design should maintain relationships between captured data and annotations that were taken at a particular 
simulation timestamps. Optionally, It should also maintain relationships between two consecutive timestamps if they 
will be used for time related perception tasks (e.g. object tracking).  

* This design should support streaming data, since the data will be created on the fly during the simulation from 
multiple processes or cloud instances. 

* This design should be able to easily support new types of sensor annotations.

* The design assumes the synthetic dataset can be serialized to filesystems. 
It does not make assumptions about transmission of storage of the dataset on a particular database management system. 
However, it’s flexible enough to support such use cases in the future. 

## Terminology

Definitions used in this document:

* **simulation**: an execution of a collection of X Unity builds (or builds with different parameters), 
each build is executed for a predefined steps of execution (e.g. executed for 10 seconds with 5fps for a total of 
50 frames). Here, a step is defined as one tick of the simulation wall-clock in Unity. 

* **capture**: a full rendering process of a Unity sensor which saved the rendered result to data files e.g.
(PNG, [pcd](https://www.online-convert.com/file-format/pcd), etc).

* **sequence**: a time-ordered collection of captures in the simulation. This is typically collected during the 
N-steps simulation in a Unity build. 

* **annotation**: data (e.g. bounding boxes or semantic segmentation) recorded that is used to describe a particular 
capture at the same timestamp. A capture might include multiple types of annotations. 

* **ego**: a GameObject moving in Unity scene with sensors (camera/LIDAR/radar) attached to it. 
For example, a camera attached to an object moving in the Unity scene. 

* **label**: a collection of string tokens (e.g. car, human.adult, etc.) that represents a particular type of Unity 
GameObject (or a parent GameObject from a hierarchy of objects) used in the simulation. 
A GameObject might have multiple labels used for different annotation purposes. 

* **coordinate systems**: there are three coordinate systems in simulation

    * **global coordinate system**: coordinate with respect to a particular fixed point in a Unity scene. 

    * **ego coordinate system**: coordinate with respect to an ego object. 
    Typically, this refers to an object moving in the Unity scene. 
    For convenience, we can use the mount point for one of the sensors on ego as the origin of ego coordinate system. 

    * **sensor coordinate system**: coordinate with respect to a sensor. 
    This is useful for ML model training for a single sensor, which can be transformed from a global coordinate system 
    and ego coordinate system. Raw value of object pose using the sensor coordinate system is rarely recorded in 
    simulation.

## design

This design is inspired from nuScenes [data format](#heading=h.ng38ehm32lgs). 
In nuScenes, data format relies on **relational database schema design**, which also assumes all tables are normalized.
 Relation design makes it harder to load data into memory when multiple joins are required to get the raw data of the 
 simulation. In this work, we relied on **document based schema design** which is a more flexible design compared to 
 relational design. It is more relaxed on normalization requirements. Some of the columns are duplicated across 
 different tables that might result in increased dataset size. Since there are a variety of tools that can work on 
 large document based datasets, denormalization is not considered a priority here.

### diagram![image alt text](image_0.png)

### captures

A capture record stores the relationship between a captured file, a collection of annotations, and extra metadata that 
describes this capture. A record is created when at least one of the sensors is triggered to make captures. 
Capture files might be generated in a distributed fashion from multiple simulation instances.

```
capture {
  id:                <str> -- UUID of the capture.
  sequence_id:       <str> -- UUID of the sequence.
  step:              <int> -- Used as the index of captures in the sequence. This field is used to maintain order of two captures within the same sequence. 
  timestamp:         <int> -- Simulation timestamp in milliseconds since the sequence started.
  sensor:            <obj> -- Attributes of the sensor. see below.
  ego:               <obj> -- Ego pose of this sample. See below.
  filename:          <str> -- A single file that stores sensor captured data. (e.g. camera_000.png, points_123.pcd, etc.)
  format:            <str> -- The format of the sensor captured file. (e.g. png, pcd, etc.)
  annotations:       [<obj>,...] [optional] -- List of all annotation objects in this capture. See below.
}
```


#### sequence, step and timestamp

In some use cases, two consecutive captures might not be related in time during simulation. For example, if we 
generate randomly placed objects in a scene for X steps of simulation. In this case, sequence, step and timestamp are 
irrelevant for the captured data. We can add a default value to the sequence, step and timestamp for these types of 
captures.

In cases where we need to maintain time order relationship between captures (e.g. a sequence of camera capture in a 10 
second video) and [metrics](#heading=h.9mpbqqwxedti), we need to add a sequence, step and timestamp to maintain the time 
ordered relationship of captures. Sequence represents the collection of any time ordered captures and annotations. 
Timestamps refer to the simulation wall clock in milliseconds since the sequence started. 
Steps are integer values which increase when a capture or metric event is triggered. 
We cannot use timestamps to synchronize between two different events because timestamps are floats and therefore make 
poor indices. Instead, we use a "step" counter which make it easy to associate metrics and captures that occur at the same 
time. Below is an illustration of how captures, metrics, timestamps and steps are synchronized. 
![image alt text](captures_steps_timestamps.png)

Since each sensor might trigger captures at different frequencies, at the same timestamp we might contain 0 to N 
captures, where N is the total number of sensors included in this scene. If two sensors are captured at the same 
timestamp, they should share the same sequence, step and timestamp value.

 

Physical camera sensors require some time to finish exposure. Physical LIDAR sensor requires some time to finish one 
360 scan. How do we define the timestamp of the sample in simulation? Following nuScene sensor 
[synchronization](https://www.nuscenes.org/data-collection) strategy, we define a reference line from ego origin 
to the ego’s "forward" traveling direction. The timestamp of the LIDAR scan is the time when the full rotation of 
the current LIDAR frame is achieved. A full rotation is defined as the 360 sweep between two consecutive times 
passing the reference line. The timestamp of the camera is the exposure trigger time in simulation.

#### capture.filenames

Typical sensor capture files might include:

* camera: 24-bit RGB PNG files
* LIDAR: [pcd](https://pointcloudlibrary.github.io/) point cloud file format

* RADAR: [pcd](https://pointcloudlibrary.github.io/) point cloud file format

* SONAR: [pcd](https://pointcloudlibrary.github.io/) point cloud file format

#### capture.ego

An ego record stores the ego status data when a sample is created. 
It includes translation, rotation, velocity and acceleration (optional) of the ego. 
It is computed with respect to the **global coordinate system** of a Unity scene.

```
ego {
  ego_id:       <str> -- Foreign key pointing to ego.id.
  translation:  <float, float, float> -- Position in meters: (x, y, z) with respect to the global coordinate system.
  rotation:     <float, float, float, float> -- Orientation as quaternion: w, x, y, z.
  velocity:     <float, float, float> -- Velocity in meters per second as v_x, v_y, v_z.
  acceleration: <float, float, float> [optional] -- Acceleration in meters per second^2 as a_x, a_y, a_z. 
}
```


#### capture.sensor

A sensor record that stored sensor attributes. Some of the attributes for might be identical for sensors with the same 
modality (e.g. field of view FOV for camera, beam density for LIDAR, etc.). Some attributes might be specific to a 
particular sensor (e.g. position calibration or camera focal length, etc) attached on a particular ego. Attributes 
might change during the simulation for domain randomization. Attributes can also include arbitrary key value pairs 
that describe other sensor properties in the simulation. Translation and rotation are computed with respect to the 
**ego coordinate system** of a given ego.

```
sensor {
  sensor_id:        <str> -- Foreign key pointing to sensor.id.
  ego_id:           <str> -- Foreign key pointing to ego.id.
  modality:         <str> {camera, lidar, radar, sonar,...} -- Sensor modality.
  translation:      <float, float, float> -- Position in meters: (x, y, z) with respect to the ego coordinate system. This is typically fixed during the simulation, but we can allow small variation for domain randomization.
  rotation:         <float, float, float, float> -- Orientation as quaternion: (w, x, y, z) with respect to ego coordinate system. This is typically fixed during the simulation, but we can allow small variation for domain randomization.
  camera_intrinsic: <3x3 float matrix> [optional] -- Intrinsic camera calibration. Empty for sensors that are not cameras.
  
  # add arbitrary optional key-value pairs for sensor attributes
}
```


reference: [camera_intrinsic](https://www.mathworks.com/help/vision/ug/camera-calibration.html#bu0ni74)

#### capture.annotation

An annotation record stores the 1 to 1 relationship between an annotation file to a capture file.
Each sensor capture might correspond to 0 to M annotation records, which stored as 1 to many relationships between 
capture and annotations in captures table. Some of the capture files might not have annotations. 
If annotations can be represented by json objects, they can be directly stored in the "values" field. 
Otherwise, we will need to store annotations in a special binary format using the “filename” field. TODO WHAT IS SPECIAL BINARY FORMAT? 

```
annotation {
  id:                    <str> -- UUID of the annotation.
  annotation_definition: <int> -- Foreign key which points to an annotation_definition.id. see below
  filename:              <str> [optional] -- Path to a single file that stores annotations. (e.g. sementic_000.png etc.)
  values:                [<obj>,...] [optional] -- List of objects that store annotation data (e.g. polygon, 2d bounding box, 3d bounding box, etc). The data should be processed according to a given annotation_definition.id.  
}
```


#### capture.annotation.filenames

Here we provide examples of annotation files.

##### semantic segmentation - grayscale image

A grayscale PNG file (left) that stores integer values (label pixel_value in [annotation spec](#annotation_definitions) 
reference table, semantic segmentation) of the labeled object at each pixel. Color images (right) are only used for 
demonstration purposes. Color images are optional annotation files to be saved in simulation.

#### ![image alt text](image_2.png)![image alt text](image_3.png)

#### capture.annotation.values

##### instance segmentation - polygon

A json object that stores collections of polygons. Each polygon record maps a tuple of (instance, label) to a list of 
K pixel coordinates that forms a polygon. This object can be directly stored in annotation.values 

```
semantic_segmentation_polygon {
  label_id:     <int> -- Integer identifier of the label
  label_name:   <str> -- String identifier of the label
  instance_id:  <str> -- UUID of the instance.
  polygon:      [<int, int>,...] -- List of points in pixel coordinates of the outer edge. Connecting these points in order should create a polygon that identifies the object. 
}
```


##### 2D bounding box

A json file that stores collections of 2D bounding boxes. Each bounding box record maps a tuple of (instance, label) 
to a set of 4 variables (x, y, width, height) that draws a bounding box. 
We follow the OpenCV 2D coordinate 
[system](https://github.com/vvvv/VL.OpenCV/wiki/Coordinate-system-conversions-between-OpenCV,-DirectX-and-vvvv#opencv) 
where the origin (0,0), (x=0, y=0) is at the top left of the image. Although 2D bounding boxes can also be encoded as 
polygon annotation, we choose to keep 2D bounding boxes as a separate format for convenience. 

```
bounding_box_2d {
  label_id:     <int> -- Integer identifier of the label
  label_name:   <str> -- String identifier of the label
  instance_id:  <str> -- UUID of the instance.
  x:            <float> -- x coordinate of the upper left corner.
  y:            <float> -- y coordinate of the upper left corner. 
  width:        <float> -- number of pixels in the x direction
  height:       <float> -- number of pixels in the y direction
}
```


##### 3D bounding box

A json file that stored collections of 3D bounding boxes. Each bounding box record maps a tuple of (instance, label) to 
translation, size and rotation that draws a 3D bounding box, as well as velocity and acceleration (optional) of the 3D 
bounding box. All location data is given with respect to the **sensor coordinate system**.

```
bounding_box_3d {
  label_id:     <int> -- Integer identifier of the label
  label_name:   <str> -- String identifier of the label
  instance_id:  <str> -- UUID of the instance.
  translation:  <float, float, float> -- 3d bounding box's center location in meters as center_x, center_y, center_z with respect to global coordinate system.
  size:         <float, float, float> -- 3d bounding box size in meters as width, length, height.
  rotation:     <float, float, float, float> -- 3d bounding box orientation as quaternion: w, x, y, z.
  velocity:     <float, float, float>  -- 3d bounding box velocity in meters per second as v_x, v_y, v_z.
  acceleration: <float, float, float> [optional] -- 3d bounding box acceleration in meters per second^2 as a_x, a_y, a_z.
}
```


#### instances (V2, WIP) TODO should we include this section? 

Although we don’t have a specific table that account for instances, it should be noted that instances should be checked against the following cases

* Consider cases for object tracking

* Consider cases not used for object tracking, so that instances do not need to be consistent across different captures/annotations. How to support instance segmentation (maybe we need to use polygon instead of pixel color)

* Stored in values of annotation and metric values

##### instance segmentation file - grayscale image (V2)

A grayscale PNG file that stores integer values of labeled instances at each pixel. 

![image alt text](image_4.png)

### metrics

Metrics store extra metadata that can be used to describe a particular sequence, capture or annotation. 
Metric records are stored as an arbitrary number (M) of key-value pairs. For a sequence metric, capture_id, 
annotation_id and step should be null. For a capture metric, annotation_id can be null. For an annotation metric, 
all four columns of sequence_id, capture_id, annotation_id and step are not null.

Metrics files might be generated in parallel from different simulation instances.

```
metric {
  capture_id:        <str> -- Foreign key which points to capture.id.
  annotation_id:     <str> -- Foreign key which points to annotation.id.
  sequence_id:       <str> -- Foreign key which points to capture.sequence_id.
  step:              <int> -- Foreign key which points to capture.step.
  metric_definition: <int> -- Foreign key which points to metric_definition.id
  values:            [<obj>,...] -- List of all metric records stored as json objects.  
}
```


### references

All reference tables are static tables during the simulation. This typically comes from the definition of the 
simulation and should be created before tasks running in parallel at different instances.

#### egos

A json file that stored collections of egos. This file is an enumeration of all egos in this simulation. A specific 
object with sensors attached to it is a commonly used ego in a driving simulation.

```
ego {
  id:           <str> -- UUID of the ego.
  description:  <str> [optional] -- Ego instance description.
}
```


#### sensors

A json file that stores a collections of sensors. This file is an enumeration of all sensors in this simulation. Each 
sensor is assigned a unique UUID. Each sensor is a part of an ego and stores the UUID of the ego as a foreign key. 
Multiple sensors of the same modality on the same ego should be assigned with a different identifier. 

```
sensor {
  id:               <str> -- UUID of the sensor.
  ego_id:           <int> -- Foreign key pointing to ego.id.
  modality:         <str> {camera, lidar, radar, sonar,...} -- Sensor modality.
  description:      <str> [optional] -- Sensor instance description.
}
```


#### annotation_definitions

A json file that stores collections of annotation specifications (annotation_definition). Each specification record 
describes a particular type of annotation file, which is assigned an integer identifier to a collection of 
specification records, stored as a list of key-value pairs of records. Each specification record explicitly stores how 
a record annotation file should be mapped back to labels or any enumeration of objects. 

For example, in semantic segmentation, an annotation file is stored as a grayscale PNG file, where each pixel value 
corresponds to some labels. We stored the mapping between pixel values and a subset of labels defined in this [table](#bookmark=id.is7luv1toraj). This allows us to support different mapping between labels to pixel values in any PNG files.

Typically, in annotation_definition, it enumerates all labels_id and label_name used in this simulation for annotation purposes. Some special cases like semantic segmentation might assign additional values (e.g. pixel value) used record mapping between label_id/label_name to pixel grayscale color in annotated PNG files.

```
annotation_definition {
  id:           <int> -- Integer identifier of the annotation definition.
  name:         <str> -- Human readable annotation spec name (e.g. sementic_segmentation, instance_segmentation, etc.) 
  description:  <str, optional> -- Description of this annotation specifications.
  format:       <str> -- The format of the annotation files. (e.g. png, json, etc.)
  spec:         [<obj>...] -- Format-specific specification for the annotation values (ex. label-value mappings for semantic segmentation images)
}

# semantic segmentation
annotation_definition.spec {        
  label_id:          <int> -- Integer identifier of the label
  label_name:        <str> -- String identifier of the label
  pixel_value:       <int> -- Grayscale pixel value
  color_pixel_value: <int, int, int> [optional] -- Color pixel value
}

# label enumeration spec, used to enumerate all labels this annotation is considered. This might be a subset of all labels used in simulation.
annotation_definition.spec {
  label_id:    <int> -- Integer identifier of the label
  label_name:  <str> -- String identifier of the label
}
```


#### metric_definitions

A json file that stores collections of metric specifications records (metric_definition). Each specification record 
describes a particular metric stored in [metrics](#metrics) values. Each metric_definition record is 
assigned a unique identifier to a collection of specification records, which is stored as a list of key-value pairs. 
The design is very similar to [annotation_definitions](#annotation_definitions).

```
metric_definition {
  id:           <int> -- Integer identifier of the metric definition.
  name:         <str> -- Human readable metric spec name (e.g. object_count, average distance, etc.) 
  description:  <str, optional> -- Description of this metric specifications.
  spec:         [<obj>...] -- Format-specific specification for the metric values
}

# label enumeration spec, used to enumerate all labels. For example, this can be used for object count metrics.
metric_definition.spec {
  label_id:    <int> -- Integer identifier of the label
  label_name:  <str> -- String identifier of the label
}
```

### schema versioning

* We use [semantic versioning](https://semver.org/).

* Version info is placed at root level of the json file that holds a collection of objects (e.g. captures.json, 
metrics.json, annotation_definitions.json,... ) Only one version used across all json files. 
It should change atomically across files if the version of the schema is updated.

* If a new metric_definition or annotation_definition is added, it will not change the schema version since it does 
not involve any schema change. 

## example TODO SHOULD WE INCLUDE THE MOCK DATA? 

A mockup of synthetic dataset according to this schema can be found 
[here](com.unity.perception/Schema/mock_data/simrun/README.md). In this mockup, we have:

* 1 ego

* 2 sensors: 1 camera and 1 LIDAR

* 19 labels

* 3 captures, 2 metrics, 1 sequence, 2 steps 

    * the first includes 1 camera capture and 1 semantic segmentation annotation.

    *   two captures, 1 camera capture and 1 LIDAR capture, are triggered at the same time. For the camera, semantic segmentation, instance segmentation and 3d bounding box annotations are provided. For the LIDAR sensor, semantic segmentation annotation of point cloud is included.

    * one of the metric events is emitted for metrics at capture level. The other one is emitted at annotation level.

* 3 types of annotations: semantic segmentation, 3d bound box and LIDAR semantic segmentation. 

* 1 type of metric: object counts

## -------------required above this line (except V2)-------------------------

## questions

* How do we represent a hierarchical relationship of labels? 

* how to make sure schema are consistent across runs? How much flexibility do we want to allow users to configure this schema? How do we combine dataset from two runs?

    * We need to reserve an id range for default annotation_definition and metric_definiton that is shipped with a simulation package. These default are essential to ensure we can run dataset statistics and evaluation. 

    * Users can specify their own definition that is separate from above

* We need some default labels and specs for each public dataset (e.g. cityscapes labels and cityscapes pixel colors). What should be user configrable and what should be kept as default?

* Think about venn diagram of taxonomy between two dataset

    * How do we merge two synthetic dataset? Tow public dataset? Synthetic dataset + Public Dataset?

* How can we maintain a "wall clock" during the simulation? If we have different sensor modules running under different processes (or some of them on GPU), can we rely on timestamp to reconstruct the sequence of captures? Do we really need extra column “step” in order to keep track of this information? 

* Should we allow customers to use their own test set? What do we want to do if customers use a spec that does not have a public dataset? How can dataset evaluation support custom label definition from customers? 
